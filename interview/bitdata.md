# 大数据

100 亿数字，怎么统计前 100 大的？

10 亿个 url，每个 url 大小小于 56B，要求去重，内存 4G。

1KW 句子算相似度（还是那套分块 + hash / 建索引，但是因为本人不是做这个的，文本处理根本说一片空白，所以就不误导大家了），之后就是一直围绕大数据的题目不断深化。

Q1：给定一个 1T 的单词文件，文件中每一行为一个单词，单词无序且有重复，当前有 5 台计算机。请问如何统计词频？

Q2：每台计算机需要计算 200G 左右的文件，内存无法存放 200G 内容，那么如何统计这些文件的词频？

Q3：如何将 1T 的文件均匀地分配给 5 台机器，且每台机器统计完词频生成的文件只需要拼接起来即可（即每台机器统计的单词不出现在其他机器中）

一个大文件 A 和一个小文件 B，里面存的是单词，要求出在文件 B 中但不在文件 A 中的单词。然后大文件 A 是无法直接存到内存中的。

一道题目是如果有一个人注册一个 qq，如何保证这个 qq 号码和之前已存在的 qq 号码不重复呢？

扔硬币，连续出现两次正面即结束，问扔的次数期望

有 100W 个集合，每个集合中的 word 是同义词，同义词具有传递性， 比如集合 1 中有 word a, 集合 2 中也有 word a, 则集合 1，2 中所有词都是同义词，对这 100W 个集合进行归并，同义词都在一个集合当中。

有几个 G 的文本，每行记录了访问 ip 的 log ，如何快速统计 ip 出现次数最高的 10 个 ip，如果只用 linux 指令又该怎么解决；

海量数据的 topk 问题
